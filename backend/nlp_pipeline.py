# nlp_pipeline.py 

from transformers import pipeline, AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F
import logging

logging.basicConfig(level=logging.INFO)
print("NLP Pipeline modÃ¼lÃ¼ yÃ¼kleniyor...")

try:
    
    sentiment_pipeline = pipeline(
        "sentiment-analysis",
        model="savasy/bert-base-turkish-sentiment-cased"
    )
    print("âœ“ Duygu analizi modeli yÃ¼klendi.")

    
    ner_pipeline = pipeline(
        "ner",
        model="savasy/bert-base-turkish-ner-cased",
        tokenizer="savasy/bert-base-turkish-ner-cased",
        aggregation_strategy="simple"
    )
    print("âœ“ NER modeli yÃ¼klendi.")

    
    sbert_model_name = "emrecan/bert-base-turkish-cased-mean-nli-stsb-tr"
    sbert_tokenizer = AutoTokenizer.from_pretrained(sbert_model_name)
    sbert_model = AutoModel.from_pretrained(sbert_model_name)
    
    def mean_pooling(model_output, attention_mask):
        """Token embedding'lerinin ortalamasÄ±nÄ± alÄ±r"""
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    def get_sentence_embedding(text):
        """Metin iÃ§in embedding vektÃ¶rÃ¼ Ã¼retir"""
        encoded_input = sbert_tokenizer(text, padding=True, truncation=True, return_tensors='pt', max_length=512)
        with torch.no_grad():
            model_output = sbert_model(**encoded_input)
        return mean_pooling(model_output, encoded_input['attention_mask'])
    
    
    KONU_ETIKETLERI = {
        "Ä°ÅŸ ve Kariyer": "iÅŸ toplantÄ± patron mÃ¼dÃ¼r proje gÃ¶rev ÅŸirket ofis maaÅŸ terfi kariyer iÅŸyeri mesai",
        "EÄŸitim ve Okul": "okul ders Ã¶ÄŸretmen sÄ±nav Ã¶dev Ã¼niversite Ã¶ÄŸrenci not eÄŸitim",
        "Sosyal Ä°liÅŸkiler": "arkadaÅŸ buluÅŸma dostluk sosyal iliÅŸki dost sohbet eÄŸlence",
        "Aile": "anne baba kardeÅŸ aile Ã§ocuk ebeveyn akraba ev yuva",
        "SaÄŸlÄ±k": "hastane doktor saÄŸlÄ±k hastalÄ±k ilaÃ§ tedavi eczane vitamin hasta tahlil muayene kilo diyet spor halsiz rahatsÄ±z aÄŸrÄ± acÄ± yorgun grip nezle",
        "Finans ve Para": "para maaÅŸ banka kredi alÄ±ÅŸveriÅŸ borÃ§ tasarruf yatÄ±rÄ±m harcama Ã¶deme market",
        "Teknoloji": "bilgisayar telefon internet uygulama yazÄ±lÄ±m teknoloji dijital online oyun",
        "KiÅŸisel GeliÅŸim": "hedef motivasyon Ã¶ÄŸrenme kitap okuma Ã¶zgÃ¼ven baÅŸarÄ± kariyer planÄ± eÄŸitim semineri",
        "Genel GÃ¼nlÃ¼k": "bugÃ¼n gÃ¼n gÃ¼nlÃ¼k rutin normal gÃ¼ndelik sabah akÅŸam"
    }
    
    
    HARD_KEYWORDS = {
        "Ä°ÅŸ ve Kariyer": ["iÅŸ", "iÅŸe", "iÅŸten", "iÅŸyeri", "patron", "patronum", "mÃ¼dÃ¼r", "toplantÄ±", "proje", "gÃ¶rev", "ofis", "mesai", "ÅŸirket"],
        "EÄŸitim ve Okul": ["okul", "okula", "okulda", "okuldan", "ders", "derste", "Ã¶ÄŸretmen", "sÄ±nav", "Ã¶dev", "Ã¼niversite", "Ã¶ÄŸrenci"],
        "Aile": ["anne", "annem", "annemi", "baba", "babam", "babamÄ±", "babamla", "kardeÅŸ", "kardeÅŸim", "aile", "Ã§ocuk", "Ã§ocuÄŸum", "ebeveyn"],
        "SaÄŸlÄ±k": ["hastane", "hastaneye", "hastanede", "hastaneden", "doktor", "doktora", "hasta", "hastalÄ±k", "ilaÃ§", "tedavi", "eczane", "vitamin", "tahlil", "aÄŸrÄ±", "acÄ±", "grip", "spor", "spora", "sporu"],
        "Finans ve Para": ["para", "parasÄ±", "maaÅŸ", "banka", "kredi", "borÃ§", "Ã¶deme", "alÄ±ÅŸveriÅŸ", "alÄ±ÅŸveriÅŸe", "market", "satÄ±n", "harcama", "Ã¼cret"],
        "Teknoloji": ["bilgisayar", "telefon", "internet", "uygulama", "yazÄ±lÄ±m", "oyun", "online"],
        "KiÅŸisel GeliÅŸim": ["hedef", "motivasyon", "baÅŸarÄ±", "geliÅŸim", "kitap", "kitabÄ±", "oku", "okuma", "okuyorum"],
    }
    
    
    CONTEXTUAL_KEYWORDS = {
        "Finans ve Para": ["aldÄ±m", "aldÄ±"],  
    }
    
    
    konu_embeddings = {}
    for konu, keywords in KONU_ETIKETLERI.items():
        konu_embeddings[konu] = F.normalize(get_sentence_embedding(keywords), p=2, dim=1)
    
    def classify_topics(text, primary_threshold=0.22, secondary_threshold=0.18):
        """
        HÄ°BRÄ°T YAKLAÅIM: Cosine similarity + Hard keyword matching + Contextual filtering
        """
        text_lower = text.lower()
        text_embedding = F.normalize(get_sentence_embedding(text), p=2, dim=1)
        
        results = []
        for konu, konu_emb in konu_embeddings.items():
            similarity = F.cosine_similarity(text_embedding, konu_emb).item()
            
            # HARD KEYWORD BOOST
            keyword_boost = 0.0
            matched_keywords = []
            if konu in HARD_KEYWORDS:
                for keyword in HARD_KEYWORDS[konu]:
                    # SUBSTRING KONTROLÃœ: Kelimenin baÅŸÄ±nda/sonunda boÅŸluk olmalÄ±
                    # Ã–rnek: "iÅŸ" kelimesi "alÄ±ÅŸveriÅŸ" iÃ§inde geÃ§mesin
                    if f" {keyword} " in f" {text_lower} " or text_lower.startswith(keyword + " ") or text_lower.endswith(" " + keyword):
                        keyword_boost = 0.15
                        matched_keywords.append(keyword)
                        break
            
            # BAÄLAMSAL FÄ°LTRE: BazÄ± keyword'ler yalnÄ±zca gÃ¼Ã§lÃ¼ semantic skorla kabul edilir
            if konu in CONTEXTUAL_KEYWORDS and keyword_boost > 0:
                for ctx_keyword in CONTEXTUAL_KEYWORDS[konu]:
                    if ctx_keyword in matched_keywords:
                        # EÄŸer bu baÄŸlamsal kelimeyse ve base skor dÃ¼ÅŸÃ¼kse, boost'u iptal et
                        if similarity < 0.18:  # Threshold yÃ¼kseltildi (0.15 â†’ 0.18)
                            keyword_boost = 0.0
                            matched_keywords = []
                            break
            
            final_score = similarity + keyword_boost
            results.append((konu, final_score, similarity, keyword_boost, matched_keywords))
        
        results.sort(key=lambda x: x[1], reverse=True)
        
        # DEBUG
        print(f"\nğŸ” KONU SKORLARI ('{text[:50]}...'):")
        for konu, final, orig, boost, keywords in results[:6]:
            boost_str = f" (+{boost:.2f} boost)" if boost > 0 else ""
            keyword_str = f" [matched: {keywords[0]}]" if keywords else ""
            print(f"  {konu}: {final:.4f} (base: {orig:.4f}{boost_str}{keyword_str})")
        
        topics = []
        
        # STRATEJÄ° 1: Primary threshold
        for konu, final_score, _, _, _ in results:
            if final_score >= primary_threshold and konu != "Genel GÃ¼nlÃ¼k":
                topics.append(konu)
                print(f"  âœ… Konu eklendi: {konu} ({final_score:.4f})")
        
        # STRATEJÄ° 2: Secondary threshold
        if not topics:
            for konu, final_score, _, _, _ in results:
                if final_score >= secondary_threshold and konu != "Genel GÃ¼nlÃ¼k":
                    topics.append(konu)
                    print(f"  â• Ä°kincil konu: {konu} ({final_score:.4f})")
        
        # STRATEJÄ° 3: Fallback
        if not topics:
            best_topic, best_score, _, _, _ = results[0]
            if best_score > 0.12:
                topics.append(best_topic)
                print(f"  âš ï¸ Fallback: {best_topic} ({best_score:.4f})")
        
        # Maksimum 3 konu
        topics = topics[:3]
        
        return topics
    
    zero_shot_pipeline = classify_topics
    print("âœ“ Sentence-BERT (Konu SÄ±nÄ±flandÄ±rma) yÃ¼klendi.")

except Exception as e:
    print(f"HATA: Modeller yÃ¼klenirken bir sorun oluÅŸtu: {e}")
    sentiment_pipeline = None
    ner_pipeline = None
    zero_shot_pipeline = None


# --- AYARLAR ---
NER_THRESHOLD = 0.70
SENTIMENT_THRESHOLD = 0.75
MIN_WORD_COUNT = 3


def analyze_text(text: str) -> dict:
    """Gelen metni duygu, NER ve konu analiziyle iÅŸler"""
    if not sentiment_pipeline or not ner_pipeline or not zero_shot_pipeline:
        print("HATA: NLP modelleri yÃ¼klenemedi, analiz yapÄ±lamÄ±yor.")
        return {"hata": "NLP modelleri yÃ¼klenemedi."}

    # --- KORUMA KALKANI ---
    stripped_text = text.strip()
    word_count = len(stripped_text.split())
    char_count = len(stripped_text)
    
    metrics = {
        "kelime_sayisi": word_count,
        "karakter_sayisi": char_count
    }

    is_question = stripped_text.endswith('?')
    is_too_short = word_count < MIN_WORD_COUNT 

    if is_question or is_too_short:
        print(f"KÄ±sa/Soru cÃ¼mlesi algÄ±landÄ±. NLP analizi atlanÄ±yor.")
        return {
            "sentiment": {"duygu": "neutral", "skor": 1.0},
            "entities": [],
            "topics": [],
            "metrics": metrics
        }
    
    # --- ANALÄ°Z ---
    try:
        # 1. DUYGU ANALÄ°ZÄ°
        sentiment_result = sentiment_pipeline(stripped_text)[0]
        original_label = sentiment_result["label"]
        original_score = float(round(sentiment_result["score"], 4))
        
        final_label = "neutral" if original_score < SENTIMENT_THRESHOLD else original_label
        sentiment = {"duygu": final_label, "skor": original_score}

        # 2. VARLIK TANIMA
        ner_result = ner_pipeline(stripped_text)
        entities = [
            {
                "varlik": entity["entity_group"],
                "metin": entity["word"],
                "skor": float(round(entity["score"], 4))
            } for entity in ner_result if entity["score"] > NER_THRESHOLD
        ]
        
        # 3. KONU SINIFLANDIRMA (Sentence-BERT)
        topics = zero_shot_pipeline(stripped_text)
        
        return {
            "sentiment": sentiment,
            "entities": entities,
            "topics": topics,
            "metrics": metrics
        }

    except Exception as e:
        print(f"Analiz sÄ±rasÄ±nda hata: {e}")
        import traceback
        traceback.print_exc()
        return {"hata": f"Metin analizi sÄ±rasÄ±nda hata: {str(e)}"}


# TEST BLOÄU
if __name__ == "__main__":
    print("\n--- NLP Pipeline Test ---")
    
    if not sentiment_pipeline or not ner_pipeline or not zero_shot_pipeline:
        print("âŒ Modeller yÃ¼klenemedi.")
    else:
        import json
        
        tests = [
            "BugÃ¼n hastaneye gidip kan deÄŸerlerimi incelettirdim. Doktor vitamin almam gerektiÄŸini sÃ¶yledi.",
            "Patronumla baÅŸarÄ±lÄ± bir toplantÄ± geÃ§irdik.",
            "Okuldaki sÄ±navÄ±m Ã§ok iyiydi, arkadaÅŸlarÄ±mla kutladÄ±k.",
            "Annem hastalandÄ±, hastaneye gittik."
        ]
        
        for test in tests:
            print(f"\nğŸ“ Metin: {test}")
            result = analyze_text(test)
            print(json.dumps(result, indent=2, ensure_ascii=False))